% !TEX root = ../../ThesisGchatzi.tex

\graphicspath{{Papers/SpringerJournalOfBigData/}}

Forecasting time series is crucially useful in various applications, such as resource demand management (e.g., water, electricity, natural gas), stock market and supply demand forecasting (e.g., in super markets). An electricity provider, for example, could forecast future demand on its power supply network, based on the historical consumption time series of its customers. This way, precautionary measures could be taken when a larger electricity demand is anticipated, to avoid a potential power outage. Depending on the dataset, time series forecasting can be a rather complex and computationally intensive task due to the high dimensionality and usual uncertainty in such data. Classical statistical methods (e.g., autoregressive integrated moving average, simple exponential smoothing), or more data-driven, machine learning-based (e.g., recursive neural networks) approaches could be employed for such a task. In this chapter, we present a data-driven $k$-Nearest Neighbors ($k$NN) method for large-scale analytics on Big Data.

During the past few years, new database management and distributed computing technologies have emerged to satisfy the need for systems that can efficiently store and operate on massive volumes of data. \textit{MapReduce}~\cite{dean2008msd}, a distributed programming model which maps operations on data elements to several machines (i.e.,~\textit{reducers}), set the foundation for this technology trend. This laid the groundwork for the development of open source distributed processing engines such as the Apache \textit{Hadoop}, \textit{Spark} and \textit{Flink}~\cite{alexandrov2014stratosphere}, that efficiently implement and extend MapReduce. These engines offer a handful of tools that operate on Big Data stored in distributed storages, supported by distributed file systems such as the \textit{Hadoop Distributed File System} (HDFS). Among them, Flink provides a mechanism for automatic procedure optimization and exhibits better performance on iterative distributed algorithms~\cite{studybig2014}. It also exhibits better overall performance, as it processes tasks in a pipelined fashion~\cite{flinkPipeline}. This allows the concurrent execution of successive tasks, i.e., a reducer can start executing as soon as it receives it's input from a mapper, without requiring all the mappers to finish first.

Data analysis and knowledge extraction from Big Data collections is often performed by applying specific machine learning techniques. The simplicity along with the effectiveness of the $k$NN algorithm, have motivated many research communities over the years with numerous applications and scientific approaches which exploit or improve its potential, especially in spatial databases and data mining. Of particular interest are the $k$NN \textit{joins} methods~\cite{bohm2004knn}, which retrieve the nearest neighbors of \textit{every} element in a testing dataset ($R$) from a set of elements in a training dataset ($S$). Each data element consists of several \textit{features}, which constitute the preliminary knowledge on which the neighbor retrieval is conducted. However, computing $k$NN joins on vast amounts of data can be very time consuming when conducted by a single CPU, as it requires computing $k$NN for each element in dataset $R$. Additionally, a possible extension of such methods to perform machine learning tasks such as classification or regression, magnifies the complexity. Various studies have been also carried out towards \textit{approximate} solutions of $k$NN, where there is a trade-off between the algorithm's precision and complexity. 

In this chapter, we introduce a framework of methods for scalable management, analysis and mining on Big Data collections. We present the \textit{Flink Machine Learning $k$NN} (FML-$k$NN for short) framework which implements a probabilistic classifier and a regressor. Specifically, we introduce a MapReduce-based version of $k$NN joins, which reduces file operations for large amounts of data and is uniquely initialized upon launch. Our approach is unified in a single session to reduce space occupation and cluster overloading. Through an experimental evaluation on real-world water consumption data, we show that the proposed method achieves high prediction precision and useful knowledge extraction.

The rest of this chapter is organized as follows. Section~\ref{sec:knn_prelim} presents some preliminaries and essential basic concepts. Section~\ref{sec:knn_methods} presents FML-$k$NN. In Section~\ref{sec:knn_experiment}, we evaluate the framework's methods in terms of wall-clock completion time and scalability. We also present and discuss two case studies on knowledge extraction tasks over large amounts of water consumption data. Finally, Section~\ref{sec:concl_knn} concludes the paper and outlines our future research directions.