During the past few years, new database management and distributed computing technologies have emerged to satisfy the need for systems that can efficiently store and operate on massive volumes of data. \textit{MapReduce}~\cite{dean2008msd}, a distributed programming model which maps operations on data elements (i.e.,~\textit{mappers}) to several machines (i.e.,~\textit{reducers}), set the foundation for this technology trend. This laid the groundwork for the development of open source distributed processing engines such as the Apache \textit{Hadoop}, \textit{Spark} and \textit{Flink}~\cite{alexandrov2014stratosphere}, that efficiently implement and extend MapReduce. These engines offer a handful of tools that operate on Big Data stored in distributed storages, supported by distributed file systems such as the \textit{Hadoop Distributed File System} (HDFS). Among them, Flink provides a mechanism for automatic procedure optimization and exhibits better performance on iterative distributed algorithms~\cite{studybig2014}. It also exhibits better overall performance, as it processes tasks in a pipelined fashion~\cite{flinkPipeline}. This allows the concurrent execution of successive tasks, i.e., a reducer can start executing as soon as it receives it's input from a mapper, without requiring all the mappers to finish first.

Data analysis and knowledge extraction from Big Data collections is often performed by applying specific machine learning techniques. The simplicity along with the effectiveness of the \textit{$k$-nearest neighbors} ($k$NN) algorithm, have motivated many research communities over the years with numerous applications and scientific approaches which exploit or improve its potential, especially in spatial databases and data mining. Of particular interest are the $k$NN \textit{joins} methods~\cite{bohm2004knn}, which retrieve the nearest neighbors of \textit{every} element in a testing dataset ($R$) from a set of elements in a training dataset ($S$). Each data element consists of several \textit{features}, which constitute the preliminary knowledge on which the neighbor retrieval is conducted. However, computing $k$NN joins on vast amounts of data can be very time consuming when conducted by a single CPU, as it requires computing $k$NN for each element in dataset $R$. Additionally, a possible extension of such methods to perform machine learning tasks such as classification or regression, magnifies the complexity. Various studies have been also carried out towards \textit{approximate} solutions of $k$NN, where there is a trade-off between the algorithm's precision and complexity. 

In this work we introduce a framework of methods for scalable data analysis and mining on Big Data collections. We present the \textit{Flink Machine Learning $k$NN} (FML-$k$NN for short) framework which implements a probabilistic classifier and a regressor. It's core algorithm is an extension of F-$zk$NN~\cite{chatzigeorgakidis2015mapreduce}, which is built upon an optimized version of the H-$zk$NNJ~\cite{zhang2012epk} distributed approximate $k$NN joins algorithm. In particular, the overall contributions of our work are as follows:
\begin{itemize}
	\item We propose a novel, easily extendible distributed processing framework that performs probabilistic classification and regression using $k$NN joins.
	\item The framework operates in a single distributed session, saving I/O and communication resources. Similar approaches require three distributed sessions.
	\item We optimize the execution of the first processing stage using Flink's \textit{broadcast sets}, contrary to F-$zk$NN, which performs expensive dataset propagation.
	%\item We make publicly available~\cite{daiadAlgs} both Flink- and Spark-based implementations that can be further developed and maintained independently, in order to motivate researchers towards further exploration activities.
	\item We present a detailed experimental and comparative evaluation with similar approaches and exhibit our framework's efficiency in terms of scalability and wall-clock completion time.
	\item We conduct experiments on two real-world cases using water consumption related datasets and extract useful knowledge and insights towards the induction of more efficient water use.
\end{itemize}

The remainder of this work is organized as follows. Section~\ref{sec:sec_relapp} reviews related work on similar approaches. Section~\ref{sec:preliminaries} presents some preliminaries and essential basic concepts. Section~\ref{sec:sec_alg} presents FML-$k$NN. In Section~\ref{sec:evaluation}, we evaluate the framework's methods in terms of wall-clock completion time and scalability. We also present and discuss two case studies on knowledge extraction tasks over large amounts of water consumption data. Finally, Section~\ref{sec:sec_conc} concludes the paper and outlines our future research directions.